# minio
2025-10-1
# Minio å®‰è£…

å¯å®‰è£…å•èŠ‚ç‚¹æˆ–åˆ†å¸ƒå¼çš„ minio , æœ¬åœ°éƒ¨ç½²æˆ–è€… docker éƒ½å¯ minio ä¸Šä¼ â¼€ä¸ª csv â½‚ä»¶, ä½¿â½¤ hive æ˜ å°„è¯¥â½‚ä»¶ä¸ºè¡¨, hive å¯æŸ¥è¯¢, èƒ½å®ç° ds åˆ†åŒºä¸ºä½³, ä½¿â½¤æ—¶é—´åˆ†åŒº, ds=20250925


### ä¸€ã€ç¯å¢ƒå‡†å¤‡

1. **å‰ç½®æ¡ä»¶**

- å·²å®‰è£… CDH å•èŠ‚ç‚¹é›†ç¾¤ï¼ˆç¡®ä¿æ“ä½œç³»ç»Ÿä¸º CentOS 7/8 æˆ– Ubuntu 18.04+ï¼‰

- å…³é—­é˜²ç«å¢™æˆ–å¼€æ”¾ MinIO æ‰€éœ€ç«¯å£ï¼ˆé»˜è®¤ 9000 ç”¨äº APIï¼Œ9001 ç”¨äºæ§åˆ¶å°ï¼‰

```bash

# CentOS å…³é—­é˜²ç«å¢™

systemctl stop firewalld

systemctl disable firewalld

# Ubuntu å…³é—­é˜²ç«å¢™

ufw disable

```

- ç¡®ä¿ç³»ç»Ÿæœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼ˆå»ºè®®å•ç‹¬æŒ‚è½½ä¸€å—ç£ç›˜ç”¨äº MinIO æ•°æ®å­˜å‚¨ï¼Œå¦‚`/data/minio`ï¼‰

### äºŒã€å®‰è£… MinIO æœåŠ¡

#### 1. åˆ›å»ºæ•°æ®ç›®å½•ä¸ç”¨æˆ·

bash

```bash

# åˆ›å»º MinIO æ•°æ®å­˜å‚¨ç›®å½•ï¼ˆå»ºè®®ä½¿ç”¨ç‹¬ç«‹ç£ç›˜æŒ‚è½½ç‚¹ï¼‰

mkdir -p /data/minio

chmod -R 777 /data/minio # èµ‹äºˆè¯»å†™æƒé™

# åˆ›å»ºä¸“ç”¨ç”¨æˆ·ï¼ˆå¯é€‰ï¼Œå¢å¼ºå®‰å…¨æ€§ï¼‰

useradd -m minio

chown -R minio:minio /data/minioï¼ˆä¸é€‰ï¼‰

```

#### 2. ä¸‹è½½ MinIO äºŒè¿›åˆ¶æ–‡ä»¶

MinIO åŸºäº Go è¯­è¨€å¼€å‘ï¼Œå•æ–‡ä»¶éƒ¨ç½²æ— éœ€ä¾èµ–ï¼Œç›´æ¥ä¸‹è½½å³å¯ï¼š

```bash

# åˆ‡æ¢åˆ° /usr/local/bin ç›®å½•ï¼ˆç³»ç»Ÿå¯æ‰§è¡Œè·¯å¾„ï¼‰

cd /usr/local/bin

# ä¸‹è½½ MinIO äºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆé€‰æ‹©å¯¹åº”ç³»ç»Ÿç‰ˆæœ¬ï¼Œè¿™é‡Œä»¥ Linux amd64 ä¸ºä¾‹ï¼‰

wget https://dl.min.io/server/minio/release/linux-amd64/minio

# èµ‹äºˆæ‰§è¡Œæƒé™

chmod +x minio

# éªŒè¯æ˜¯å¦å¯æ‰§è¡Œ

minio --version # è¾“å‡ºç±»ä¼¼ï¼šminio version RELEASE.2024-05-28T17-19-04Z

```

#### 3. é…ç½®ç¯å¢ƒå˜é‡ï¼ˆå¯†é’¥ä¸å­˜å‚¨è·¯å¾„ï¼‰

```bash

# åˆ›å»º MinIO é…ç½®æ–‡ä»¶

vi /etc/profile.d/minio.sh

# æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼ˆè®¾ç½®ç®¡ç†å‘˜è´¦å·å¯†ç å’Œæ•°æ®ç›®å½•ï¼‰

export MINIO_ROOT_USER="minioadmin" # ç®¡ç†ç”¨æˆ·å

export MINIO_ROOT_PASSWORD="minioadmin" # ç®¡ç†å¯†ç ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®å¤æ‚å¯†ç ï¼‰

export MINIO_VOLUMES="/data/minio" # æ•°æ®å­˜å‚¨ç›®å½•

export MINIO_OPTS="--console-address :9001" # æ§åˆ¶å°ç«¯å£ï¼ˆAPI ç«¯å£é»˜è®¤ 9000ï¼‰

# ä½¿é…ç½®ç”Ÿæ•ˆ

source /etc/profile.d/minio.sh

```

### ä¸‰ã€é…ç½®ç³»ç»ŸæœåŠ¡ï¼ˆå¼€æœºè‡ªå¯ï¼‰

ä¸ºä¾¿äºç®¡ç†ï¼Œå°† MinIO æ³¨å†Œä¸ºç³»ç»ŸæœåŠ¡ï¼š

#### 1. åˆ›å»ºæœåŠ¡æ–‡ä»¶

```bash

vi /etc/systemd/system/minio.service

```

#### 2. å†™å…¥æœåŠ¡é…ç½®

```ini

ï¼ˆä¸è¦ç”¨è¿™ä¸ªï¼‰

[Unit]

Description=MinIO Object Storage Service

Documentation=https://min.io/docs/minio/linux/index.html

Wants=network-online.target

After=network-online.target

[Service]

User=minio # è¿è¡Œç”¨æˆ·ï¼ˆè‹¥åˆ›å»ºäº†ä¸“ç”¨ç”¨æˆ·ï¼‰

Group=minio # è¿è¡Œç”¨æˆ·ç»„

EnvironmentFile=/etc/profile.d/minio.sh # å¼•ç”¨ç¯å¢ƒå˜é‡é…ç½®

ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES

Restart=always # æ•…éšœè‡ªåŠ¨é‡å¯

RestartSec=5 # é‡å¯é—´éš” 5 ç§’

[Install]

WantedBy=multi-user.target

ï¼ˆä½¿ç”¨è¿™ä¸ªæ–‡ä»¶ï¼‰

[Unit]

Description=MinIO Object Storage Service

Documentation=https://min.io/docs/minio/linux/index.html

Wants=network-online.target

After=network-online.target

[Service]

# ç”¨ root å¯åŠ¨ï¼Œä¸å†æŒ‡å®š minio ç”¨æˆ·

ExecStart=/usr/local/bin/minio server /data --console-address ":9001"

Restart=always

RestartSec=5

LimitNOFILE=65536

Environment="MINIO_ROOT_USER=admin"

Environment="MINIO_ROOT_PASSWORD=admin123456"

[Install]

WantedBy=multi-user.target

```

1. ç¡®è®¤ MinIO äºŒè¿›åˆ¶æ˜¯å¦å­˜åœ¨

`which minio ls -lh /usr/local/bin/minio`

å¦‚æœæ²¡æœ‰ï¼Œå°±å»ä¸‹è½½æœ€æ–°ç‰ˆæœ¬ï¼š

```bash

`wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio mv minio /usr/local/bin/`

```

#### 3. å¯åŠ¨å¹¶è®¾ç½®è‡ªå¯

```bash

# é‡è½½ç³»ç»ŸæœåŠ¡

systemctl daemon-reload

# å¯åŠ¨ MinIO

systemctl start minio

# æŸ¥çœ‹çŠ¶æ€ï¼ˆç¡®ä¿ Active: active (running)ï¼‰

systemctl status minio

# è®¾ç½®å¼€æœºè‡ªå¯

systemctl enable minio

```

### å››ã€è®¿é—® MinIO æ§åˆ¶å°

1. **é€šè¿‡æµè§ˆå™¨è®¿é—®**æ‰“å¼€`http://CDHèŠ‚ç‚¹IP:9001`ï¼Œä½¿ç”¨é…ç½®çš„è´¦å·å¯†ç ï¼ˆ`minioadmin/minioadmin`ï¼‰ç™»å½•ã€‚

2. **åˆ›å»ºå­˜å‚¨æ¡¶ï¼ˆBucketï¼‰**

- ç™»å½•åç‚¹å‡»å·¦ä¾§**Buckets**â†’**Create Bucket**

- è¾“å…¥æ¡¶åï¼ˆå¦‚`cdh-minio-bucket`ï¼‰ï¼Œä¿æŒé»˜è®¤é…ç½®ï¼Œç‚¹å‡»**Create**ã€‚

### äº”ã€CDH é›†æˆ MinIOï¼ˆå¯é€‰ï¼‰

è‹¥éœ€è®© CDH ç»„ä»¶ï¼ˆå¦‚ Sparkã€Hiveï¼‰è®¿é—® MinIOï¼Œéœ€é…ç½® S3 å…¼å®¹å‚æ•°ï¼š

#### 1. é…ç½® Core Site

åœ¨ CDH ç®¡ç†ç•Œé¢ï¼ˆCloudera Managerï¼‰â†’**HDFS**â†’**é…ç½®**â†’ æœç´¢`core-site.xml`ï¼Œæ·»åŠ ä»¥ä¸‹å±æ€§ï¼š

```xml

<property>

<name>fs.s3a.endpoint</name>

<value>http://CDHèŠ‚ç‚¹IP:9000</value> <!-- MinIO API åœ°å€ -->

</property>

<property>

<name>fs.s3a.access.key</name>

<value>minioadmin</value> <!-- MinIO ç”¨æˆ·å -->

</property>

<property>

<name>fs.s3a.secret.key</name>

<value>minioadmin</value> <!-- MinIO å¯†ç  -->

</property>

<property>

<name>fs.s3a.path.style.access</name>

<value>true</value> <!-- å¯ç”¨è·¯å¾„é£æ ¼è®¿é—®ï¼ˆé¿å… DNS é…ç½®ï¼‰ -->

</property>

<property>

<name>fs.s3a.connection.ssl.enabled</name>

<value>false</value> <!-- å•èŠ‚ç‚¹ç¯å¢ƒå¯å…³é—­ SSL -->

</property>

```

#### 2. éªŒè¯é›†æˆ

é€šè¿‡ HDFS å‘½ä»¤è®¿é—® MinIOï¼š

```bash

# åˆ—å‡º MinIO ä¸­çš„æ¡¶

hadoop fs -ls s3a://

# ä¸Šä¼ æ–‡ä»¶åˆ° MinIO æ¡¶

hadoop fs -put /etc/profile s3a://cdh-minio-bucket/

```

### å…­ã€å¸¸è§é—®é¢˜æ’æŸ¥

1. **æ— æ³•è®¿é—®æ§åˆ¶å°**

- æ£€æŸ¥ç«¯å£æ˜¯å¦å¼€æ”¾ï¼š`netstat -tulpn | grep 9001` æˆ– `ss -tulpn | grep 9001

`

- æŸ¥çœ‹æ—¥å¿—ï¼š`journalctl -u minio -f`

2. **CDH ç»„ä»¶è®¿é—®å¤±è´¥**

- ç¡®è®¤`core-site.xml`é…ç½®ä¸­çš„ endpointã€è´¦å·å¯†ç æ˜¯å¦æ­£ç¡®

- æµ‹è¯•ç½‘ç»œè¿é€šæ€§ï¼š`curl http://CDHèŠ‚ç‚¹IP:9000`ï¼ˆåº”è¿”å› XML æ ¼å¼çš„é”™è¯¯ä¿¡æ¯ï¼‰

3. **æƒé™é—®é¢˜**

- ç¡®ä¿æ•°æ®ç›®å½•`/data/minio`æƒé™æ­£ç¡®ï¼ˆ`chmod 777`æˆ–å½’å± minio ç”¨æˆ·ï¼‰

é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œå³å¯åœ¨ CDH å•èŠ‚ç‚¹ç¯å¢ƒä¸­å®Œæˆ MinIO å®‰è£…ï¼Œå¹¶å®ç°ä¸ Hadoop ç”Ÿæ€çš„é›†æˆï¼Œç”¨äºå­˜å‚¨éç»“æ„åŒ–æ•°æ®æˆ–ä½œä¸ºè®¡ç®—æ¡†æ¶çš„è¾“å…¥è¾“å‡ºå­˜å‚¨ã€‚

## minio ä¸Šä¼ â¼€ä¸ª csv â½‚ä»¶, ä½¿â½¤ hive æ˜ å°„è¯¥â½‚ä»¶ä¸ºè¡¨, hive å¯æŸ¥è¯¢, èƒ½å®ç° ds åˆ†åŒºä¸ºä½³, ä½¿â½¤æ—¶é—´åˆ†åŒº, ds=20250925

``` sql

CREATE EXTERNAL TABLE my_csv_table 

( 

CRIM double, 

ZN double, 

INDUS double, 

CHAS double, 

NOX double, 

RM double, 

AGE double, 

DIS double, 

RAD double, 

TAX double, 

PTRATIO double, 

B double, 

LSTAT double, 

TARGET double 

) 

PARTITIONED BY (ds STRING) 

ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' 

STORED AS TEXTFILE 

LOCATION 's3a://first-bucket/'; 

ALTER TABLE my_csv_table ADD PARTITION (ds='20250925') 

LOCATION 's3a://first-bucket/ds=20250925/'; 

SELECT * FROM my_csv_table WHERE ds='20250925'; 

MSCK REPAIR TABLE my_csv_table;


```






#  2. CDH å®‰è£… hbase
# åˆ›å»º hbase è¡¨, æ•°æ®ç”± Flink å†™â¼Š FlinkAPI æˆ–è€… FlinkSQL éƒ½å¯ä»¥
#  hbase rowkey çš„è®¾è®¡, ç¬¦åˆä¼ä¸šåŸåˆ™
# hbase æ•°æ®è¿›â¼Šå, ä½¿â½¤ hive å’Œ hbase çš„æ˜ å°„è¡¨è¿›â¾æ˜ å°„, æ•°æ®éœ€è¦å¯æŸ¥è¯¢, å®ç°å¯åˆ†åŒºä¸ºä½³


```
1. ç™»å½• cdh1
    
    bash
    
    
    
    ```bash
    ssh root@192.168.200.30
    ```
    
2. ä¿®å¤ classpathï¼ˆåªåšä¸€æ¬¡ï¼‰
    
    bash
    
    
    
    ```bash
    echo 'export HBASE_CLASSPATH=$(find /opt/cloudera/parcels/CDH -name "*.jar" | tr '\n' ':')' >> ~/.bashrc
    source ~/.bashrc
    ```
    
3. è¿› hbase shell
    
    bash
    
    
    
    ```bash
    hbase shell
    ```
    
4. ä¾æ¬¡ç²˜è´´ï¼ˆ**# åé¢æ˜¯æ³¨é‡Šï¼Œä¸ç”¨ç²˜**ï¼‰
    
    ruby
    
    
    
    ```ruby
    # 1. å‘½åç©ºé—´
    create_namespace 'dim'
    
    # 2. å»ºè¡¨ï¼ˆæ•£åˆ—+ds åç¼€ï¼Œé¢„åˆ†åŒº 16 ä¸ªï¼‰
    create 'dim:dim_activity_full',
           {NAME => 'f', COMPRESSION => 'SNAPPY', VERSIONS => 1},
           {SPLITS => ['1000|','2000|','3000|','4000|','5000|','6000|','7000|','8000|','9000|','a000|','b000|','c000|','d000|','e000|','f000|']}
    
    # 3. éªŒè¯
    describe 'dim:dim_activity_full'
    list_namespace_tables 'dim'
    ```
    
5. çœ‹åˆ° **15 ä¸ª split** å’Œ **SNAPPY** å³æˆåŠŸï¼Œè¾“å…¥
    
    ruby
    
    
    
    ```ruby
    exit
    ```
    

---

2. ç¬¬äºŒæ­¥ï¼šåœ¨ cdh1 å»º Hive æ˜ å°„è¡¨ï¼ˆ3 åˆ†é’Ÿï¼‰
    

---

1. è¿˜åœ¨ cdh1ï¼Œåˆ‡åˆ° hive ç”¨æˆ·ï¼ˆroot ä¹Ÿå¯ï¼‰
    
    bash
    
    
    
    ```bash
    beeline -u jdbc:hive2://cdh1:10000 -n hive
    ```
    
2. ä¸€æ¬¡æ€§ç²˜è´´
    
    sql
    
    
    
    ```sql
    CREATE EXTERNAL TABLE dim_activity_full_hbase(
      activity_rule_id    string,
      activity_id         string,
      activity_name       string,
      activity_type_code  string,
      activity_type_name  string,
      activity_desc       string,
      start_time          string,
      end_time            string,
      create_time         string,
      condition_amount    decimal(16,2),
      condition_num       bigint,
      benefit_amount      decimal(16,2),
      benefit_discount    decimal(16,2),
      benefit_rule        string,
      benefit_level       string
    )
    PARTITIONED BY (ds string)
    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
    WITH SERDEPROPERTIES (
      "hbase.columns.mapping" =
      ":key,f:activity_id,f:activity_name,f:activity_type_code,f:activity_type_name,f:activity_desc,f:start_time,f:end_time,f:create_time,f:condition_amount,f:condition_num,f:benefit_amount,f:benefit_discount,f:benefit_rule,f:benefit_level"
    )
    TBLPROPERTIES (
      "hbase.table.name" = "dim:dim_activity_full",
      "hive.hbase.rowkey.part" = "2",
      "hive.hbase.rowkey.separator" = "|"
    );
    ```
    
3. é€€å‡º beeline
    
    sql
    
    
    
    ```sql
    !quit
    ```
    

---

3. ç¬¬ä¸‰æ­¥ï¼šåœ¨ cdh1 æäº¤ Flink ä½œä¸šï¼ˆ15 åˆ†é’Ÿï¼‰
    

---

3.1 å‡†å¤‡æºç ï¼ˆcdh1 ä»»æ„ç›®å½•ï¼‰

bash



```bash
mkdir -p ~/flink_job && cd ~/flink_job
```

3.2 æ–°å»º `ActivityToHBaseSQL.java`

java



```java
import org.apache.flink.table.api.*;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class ActivityToHBaseSQL {
    public static void main(String[] args) {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.enableCheckpointing(30_000);
        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

        // 1. Kafka æº
        tEnv.executeSql("CREATE TABLE kafka_activity ("
                + "  activity_rule_id STRING,"
                + "  activity_id      STRING,"
                + "  activity_name    STRING,"
                + "  activity_type_code STRING,"
                + "  activity_type_name STRING,"
                + "  activity_desc    STRING,"
                + "  start_time       STRING,"
                + "  end_time         STRING,"
                + "  create_time      STRING,"
                + "  condition_amount DECIMAL(16,2),"
                + "  condition_num    BIGINT,"
                + "  benefit_amount   DECIMAL(16,2),"
                + "  benefit_discount DECIMAL(16,2),"
                + "  benefit_rule     STRING,"
                + "  benefit_level    STRING,"
                + "  ds               STRING"
                + ") WITH ("
                + "  'connector' = 'kafka',"
                + "  'topic' = 'activity_topic',"
                + "  'properties.bootstrap.servers' = '192.168.200.30:9092,192.168.200.31:9092,192.168.200.32:9092',"
                + "  'format' = 'json'"
                + ")");

        // 2. HBase ç›®æ ‡
        tEnv.executeSql("CREATE TABLE dim_activity_full_hbase ("
                + "  rk STRING,"
                + "  f  ROW<activity_id STRING, activity_name STRING, activity_type_code STRING, activity_type_name STRING, activity_desc STRING, start_time STRING, end_time STRING, create_time STRING, condition_amount DECIMAL(16,2), condition_num BIGINT, benefit_amount DECIMAL(16,2), benefit_discount DECIMAL(16,2), benefit_rule STRING, benefit_level STRING>,"
                + "  PRIMARY KEY (rk) NOT ENFORCED"
                + ") WITH ("
                + "  'connector' = 'hbase-1.4',"
                + "  'table-name' = 'dim:dim_activity_full',"
                + "  'zookeeper.quorum' = '192.168.200.30:2181,192.168.200.31:2181,192.168.200.32:2181',"
                + "  'zookeeper.znode.parent' = '/hbase',"
                + "  'sink.buffer-flush.max-rows' = '500',"
                + "  'sink.buffer-flush.interval' = '1s'"
                + ")");

        // 3. å†™å…¥
        tEnv.executeSql("INSERT INTO dim_activity_full_hbase "
                + "SELECT "
                + "  LPAD(LOWER(HEX(MD5(activity_rule_id))),4,'0') || '|' || activity_rule_id || '|' || ds AS rk,"
                + "  ROW(activity_id, activity_name, activity_type_code, activity_type_name, activity_desc, start_time, end_time, create_time, condition_amount, condition_num, benefit_amount, benefit_discount, benefit_rule, benefit_level) AS f "
                + "FROM kafka_activity");
    }
}
```

3.3 ç¼–è¯‘ï¼ˆç”¨ CDH è‡ªå¸¦çš„ flink-shaded-hbaseï¼‰

bash



```bash
# æŠŠä¾èµ– jar æ”¾åˆ° lib ç›®å½•
cp /opt/cloudera/parcels/CDH/jars/hbase-* ~/flink_job/lib/
cp /opt/cloudera/parcels/FLINK/lib/flink-table-* ~/flink_job/lib/

# ç¼–è¯‘
javac -cp "lib/*" ActivityToHBaseSQL.java
jar -cvf activity.jar ActivityToHBaseSQL.class
```

3.4 æäº¤ä½œä¸šï¼ˆä»åœ¨ cdh1ï¼‰

bash



```bash
export HADOOP_CONF_DIR=/etc/hadoop/conf
flink run -m yarn-cluster -ynm activity2hbase -ys 2 -yjm 1024 -ytm 2048 activity.jar
```

çœ‹åˆ° **"Submitted application application_xxx"** å³æˆåŠŸã€‚

---

4. ç¬¬å››æ­¥ï¼šå‘æµ‹è¯•æ•°æ®ï¼ˆä»»æ„èŠ‚ç‚¹ï¼Œ2 åˆ†é’Ÿï¼‰


---

bash
```bash
kafka-console-producer --broker-list 192.168.200.30:9092 --topic activity_topic
```

ç²˜ä¸€æ¡ï¼š

JSON



```json
{"activity_rule_id":"rule_123","activity_id":"act_456","activity_name":"åŒ11æ»¡å‡","activity_type_code":"COUPON","activity_type_name":"ä¼˜æƒ åˆ¸","activity_desc":"æ»¡1000å‡200","start_time":"2025-10-01 00:00:00","end_time":"2025-10-07 23:59:59","create_time":"2025-09-30 12:00:00","condition_amount":1000.00,"condition_num":1,"benefit_amount":200.00,"benefit_discount":null,"benefit_rule":"CASH","benefit_level":"A","ds":"2025-10-06"}
```

Ctrl+C é€€å‡ºã€‚

---

5. ç¬¬äº”æ­¥ï¼šHive æŸ¥è¯¢ï¼ˆ1 åˆ†é’Ÿï¼‰


---

1. åŠ åˆ†åŒº

   bash
    ```bash
    beeline -u jdbc:hive2://cdh1:10000 -n hive -e "ALTER TABLE dim_activity_full_hbase ADD PARTITION (ds='2025-10-06');"
    ```

2. æŸ¥è¯¢

   sql
    ```sql
    beeline -u jdbc:hive2://cdh1:10000 -n hive
    SELECT * FROM dim_activity_full_hbase WHERE ds='2025-10-06' AND activity_type_code='COUPON';
    ```

   èƒ½è¿”å›åˆšæ‰é‚£æ¡æ•°æ®å³ **å…¨æµç¨‹æ‰“é€š**ã€‚




# 3. python spider çˆ¬â¾

# 3.1 çˆ¬å–ä¸­å›½â½“è±¡æ•°æ®, æ•°æ®è¦æ±‚ 10 s æ›´æ–°â¼€æ¬¡ï¼Œå¢é‡æ›´æ–°

# 3.2 çˆ¬å–ä¸­å›½å¤–æ±‡å½“â½‡å¸‚åœºæ•°æ®æ•°æ® 10 s æ›´æ–°â¼€æ¬¡ï¼Œå¢é‡æ›´æ–°

# ps : ç¯å¢ƒå¯ç§»æ¤, ä»£ç éœ€è¦åœ¨ linux ä¸­å¯è¿â¾, ä½¿â½¤ conda ç¯å¢ƒè¿›â¾é…ç½®, é…ç½®åçˆ¬â¼¿æ®µè¿›â¾é¢„é˜²
```

## 3.1 çˆ¬å–ä¸­å›½â½“è±¡æ•°æ®, æ•°æ®è¦æ±‚ 10 s æ›´æ–°â¼€æ¬¡ï¼Œå¢é‡æ›´æ–°

"""
é‚“å· 7 å¤©å¤©æ°” Â· æ¯ 10 ç§’å¢é‡æ›´æ–°ï¼ˆå…¼å®¹ PG<9.5ï¼‰
"""
import time, signal, sys
import requests, psycopg2, re
from lxml import etree

# ---------- æ•°æ®åº“ ----------
DB_CFG = dict(
    dbname='postgres',
    user='postgres',
    password='root',
    host='192.168.200.30',
    port=5432
)

# ---------- SQL ----------
CREATE_TABLE_SQL = """
CREATE TABLE IF NOT EXISTS public.weather7days (
    date        DATE PRIMARY KEY,
    weekday     VARCHAR(3),
    weather     VARCHAR(10),
    wind_dir    VARCHAR(10),
    wind_lvl    VARCHAR(10),
    temp_high   SMALLINT,
    temp_low    SMALLINT
);
"""

# ä½ç‰ˆæœ¬å…¼å®¹ï¼šå…ˆæ›´æ–°ï¼Œæ²¡å‘½ä¸­å†æ’å…¥
UPSERT_SQL = """
UPDATE public.weather7days
SET weekday     = %(weekday)s,
    weather     = %(weather)s,
    wind_dir    = %(wind_dir)s,
    wind_lvl    = %(wind_lvl)s,
    temp_high   = %(temp_high)s,
    temp_low    = %(temp_low)s
WHERE date = %(date)s;
INSERT INTO public.weather7days
(date, weekday, weather, wind_dir, wind_lvl, temp_high, temp_low)
SELECT %(date)s, %(weekday)s, %(weather)s, %(wind_dir)s, %(wind_lvl)s,
       %(temp_high)s, %(temp_low)s
WHERE NOT EXISTS (SELECT 1 FROM public.weather7days WHERE date = %(date)s);
"""

# ---------- æŠ“å– ----------
def crawl():
    url = 'https://weather.cma.cn/web/weather/57178.html'
    headers = {'User-Agent': 'Mozilla/5.0'}
    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()
    resp.encoding = 'utf-8'

    txt = ''.join(etree.HTML(resp.text)
                  .xpath('/html/body/div[1]/div[2]/div[1]/div[1]/div[2]')[0]
                  .itertext())

    pat = re.compile(r'æ˜ŸæœŸ([\u4e00-\u9fa5])\s*(\d{2}/\d{2})\s*'
                     r'([\u4e00-\u9fa5]+)\s*'
                     r'([\u4e00-\u9fa5]{2}é£)\s*'
                     r'([\u4e00-\u9fa5\d~çº§]+)\s*'
                     r'(\d{1,2})â„ƒ\s*(\d{1,2})â„ƒ', re.S)

    rows = []
    for w, d, we, wd, wl, h, l in pat.findall(txt):
        mon, day = d.split('/')
        rows.append(dict(
            date=f'2025-{int(mon):02d}-{int(day):02d}',
            weekday=w,
            weather=we,
            wind_dir=wd,
            wind_lvl=wl,
            temp_high=int(h),
            temp_low=int(l)
        ))
    return rows

# ---------- ä¸»å¾ªç¯ ----------
def main():
    with psycopg2.connect(**DB_CFG) as conn:
        with conn.cursor() as cur:
            cur.execute(CREATE_TABLE_SQL)

    while True:
        try:
            t0 = time.time()
            data = crawl()
            with psycopg2.connect(**DB_CFG) as conn:
                with conn.cursor() as cur:
                    for row in data:
                        cur.execute(UPSERT_SQL, row)
                    print(f'{time.strftime("%Y-%m-%d %H:%M:%S")} '
                          f'å·²æ›´æ–° {len(data)} æ¡è®°å½•')
            time.sleep(max(0, 10 - (time.time() - t0)))
        except KeyboardInterrupt:
            print('ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡º')
            sys.exit(0)
        except Exception as e:
            print('æŠ“å–å¼‚å¸¸ï¼š', e)
            time.sleep(10)

if __name__ == '__main__':
    signal.signal(signal.SIGINT, lambda *args: sys.exit(0))
    main()
```
æˆ‘å°†çˆ¬å–çš„æ•°æ®å­˜å‚¨åˆ°pgé‡Œï¼Œç„¶åæ²¡åç§’å¢é‡åŒæ­¥







# 3.2 çˆ¬å–ä¸­å›½å¤–æ±‡å½“â½‡å¸‚åœºæ•°æ®æ•°æ® 10 s æ›´æ–°â¼€æ¬¡ï¼Œå¢é‡æ›´æ–°

```
> **æ¯ 10 ç§’æŠ“å–ä¸€æ¬¡æ‹›å•†é“¶è¡Œå¤–æ±‡ç‰Œä»· APIï¼ˆ`https://fx.cmbchina.com/api/v1/fx/rate`ï¼‰å¹¶å†™å…¥ PostgreSQL æ•°æ®åº“ã€‚**

ç‰¹ç‚¹ï¼š

- è‡ªåŠ¨å»ºåº“å»ºè¡¨ï¼›
    
- ä½ç‰ˆæœ¬ PostgreSQL å…¼å®¹ï¼ˆä¸ä¾èµ– `ON CONFLICT`ï¼‰ï¼›
    
- æ¯æ¡æ•°æ®åŒ…å«å¸ç§ã€ä¸­æ–‡åã€æ—¶é—´æˆ³ã€å®Œæ•´ JSONï¼›
    
- æ¯ 10 ç§’è‡ªåŠ¨æ›´æ–°ï¼›
    
- å¸¦æ—¥å¿—è®°å½•ã€å¼‚å¸¸æ•è·ã€‚
    

---

## ğŸ—ï¸ äºŒã€æ¨¡å—ç»“æ„åˆ†è§£

### 1ï¸âƒ£ æ—¥å¿—é…ç½®

`logging.basicConfig(     level=logging.INFO,     format='%(asctime)s - %(levelname)s - %(message)s',     handlers=[         logging.FileHandler('spider_amap_weather.log'),         logging.StreamHandler()     ] )`

- åŒæ—¶å†™å…¥æ–‡ä»¶ `spider_amap_weather.log` å’Œç»ˆç«¯ï¼›
    
- æ ¼å¼ï¼š`æ—¶é—´ - æ—¥å¿—çº§åˆ« - æ¶ˆæ¯`;
    
- å…¨å±€æ—¥å¿—å¯¹è±¡ `logger` ç”¨äºåç»­è¾“å‡ºã€‚
    

---

### 2ï¸âƒ£ æ•°æ®åº“é…ç½®

`DB_CFG = dict(     host='192.168.200.30',     port=5432,     database='spider_db',     user='postgres',     password='root' )`

ç®€å•æ˜äº†ï¼Œä¾¿äºæ”¹ç¯å¢ƒæ—¶ç»Ÿä¸€ç»´æŠ¤ã€‚

---

## ğŸ§± ä¸‰ã€æ•°æ®åº“åˆå§‹åŒ–å‡½æ•° `init_db()`

è¿™éƒ¨åˆ†éå¸¸å®ç”¨ï¼Œè‡ªåŠ¨å¤„ç†æ•°æ®åº“ä¸å­˜åœ¨çš„æƒ…å†µ ğŸ‘‡

`def init_db():     init = {**DB_CFG, 'database': 'postgres'}     conn = psycopg2.connect(**init)     conn.set_session(autocommit=True)`

è¿™é‡Œè¿çš„æ˜¯é»˜è®¤æ•°æ®åº“ `postgres`ï¼ˆç³»ç»Ÿåº“ï¼‰ï¼Œå› ä¸ºç›®æ ‡æ•°æ®åº“ `spider_db` å¯èƒ½è¿˜æ²¡åˆ›å»ºã€‚

---

### 3.1 æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨

`cur.execute("SELECT 1 FROM pg_database WHERE datname='spider_db';") if not cur.fetchone():     cur.execute("CREATE DATABASE spider_db;")     logger.info('å·²è‡ªåŠ¨åˆ›å»ºæ•°æ®åº“ spider_db')`

- å¦‚æœæ•°æ®åº“ä¸å­˜åœ¨åˆ™è‡ªåŠ¨ `CREATE DATABASE`ï¼›
    
- å¹¶æ‰“å°æ—¥å¿—æç¤ºã€‚
    

---

### 3.2 åˆ é™¤æ—§è¡¨å¹¶é‡å»º

`with psycopg2.connect(**DB_CFG) as conn:     with conn.cursor() as cur:         cur.execute("DROP TABLE IF EXISTS public.spider_exchange_rate_dtl;")         cur.execute("""             CREATE TABLE public.spider_exchange_rate_dtl (                 id      BIGSERIAL PRIMARY KEY,                 ccy_zh  VARCHAR(20) NOT NULL,                 ccy_en  VARCHAR(3)  NOT NULL,                 ct_time TIMESTAMP   NOT NULL,                 data    TEXT        NOT NULL             );         """)         cur.execute("""             CREATE UNIQUE INDEX uk_ccy_time             ON public.spider_exchange_rate_dtl (ccy_en, ct_time);         """)`

ğŸ“Œ **ä½œç”¨ï¼š**

- åˆ é™¤æ—§è¡¨ï¼Œé˜²æ­¢ç»“æ„ä¸ä¸€è‡´ï¼›
    
- é‡æ–°å»ºè¡¨ï¼›
    
- åˆ›å»ºå”¯ä¸€ç´¢å¼• `(ccy_en, ct_time)`ï¼Œä¿è¯æ¯ä¸ªå¸ç§æ¯ä¸ªæ—¶é—´ç‚¹å”¯ä¸€ã€‚
    

---

## ğŸŒ å››ã€æŠ“å–å‡½æ•° `fetch()`

### 4.1 è¯·æ±‚æ¥å£

`url = 'https://fx.cmbchina.com/api/v1/fx/rate' resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10) resp.raise_for_status() body = resp.json()['body']`

- è°ƒç”¨æ‹›å•†é“¶è¡Œå¤–æ±‡APIï¼›
    
- æ£€æŸ¥HTTPçŠ¶æ€ï¼›
    
- è§£æJSONï¼›
    
- æå–ä¸»æ•°æ®ä½“ `body`ï¼ˆåˆ—è¡¨ï¼‰ã€‚
    

---

### 4.2 æ•°æ®è§£æ

`for i in body:     date_part = i['ratDat'].replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '')     ct_time = f"{date_part} {i['ratTim']}"     res.append((         i['ccyNbr'],         i['ccyNbrEng'].split()[-1],         ct_time,         json.dumps(i, ensure_ascii=False)     ))`

æå–å­—æ®µï¼š

|å­—æ®µ|è¯´æ˜|
|---|---|
|`ccy_zh`|ä¸­æ–‡å¸ç§å|
|`ccy_en`|è‹±æ–‡å¸ç§ï¼ˆå–è‹±æ–‡åæœ€åä¸€ä¸ªå•è¯ï¼‰|
|`ct_time`|åˆå¹¶æ—¥æœŸ + æ—¶é—´å­—ç¬¦ä¸²|
|`data`|åŸå§‹JSONåºåˆ—åŒ–ï¼ˆä¿ç•™å…¨éƒ¨å­—æ®µï¼‰|

---

### 4.3 æ’å…¥æˆ–æ›´æ–°ï¼ˆä½ç‰ˆæœ¬å…¼å®¹å†™æ³•ï¼‰

`for row in res:     cur.execute("""         UPDATE public.spider_exchange_rate_dtl         SET ccy_zh = %s, data = %s         WHERE ccy_en = %s AND ct_time = %s;     """, (row[0], row[3], row[1], row[2]))      cur.execute("""         INSERT INTO public.spider_exchange_rate_dtl (ccy_zh, ccy_en, ct_time, data)         SELECT %s, %s, %s, %s         WHERE NOT EXISTS (             SELECT 1 FROM public.spider_exchange_rate_dtl             WHERE ccy_en = %s AND ct_time = %s         );     """, row + (row[1], row[2]))`

ğŸ”¥ é‡ç‚¹ï¼š

- **æ²¡æœ‰ä½¿ç”¨ PostgreSQL 9.5+ çš„ `ON CONFLICT DO UPDATE` è¯­æ³•ï¼Œ**
    
- è€Œæ˜¯æ‰‹åŠ¨å†™äº†ä¸¤æ­¥ï¼š
    
    1. `UPDATE`ï¼ˆè‹¥å­˜åœ¨åˆ™æ›´æ–°ï¼‰ï¼›
        
    2. `INSERT ... WHERE NOT EXISTS`ï¼ˆè‹¥ä¸å­˜åœ¨åˆ™æ’å…¥ï¼‰ã€‚
        

âœ… å…¼å®¹ PostgreSQL 9.2/9.3/9.4 ç­‰æ—§ç‰ˆæœ¬ã€‚

---

## äº”ã€å¾ªç¯é€»è¾‘

`while True:     t0 = time.time()     fetch()     time.sleep(max(0, 10 - (time.time() - t0)))`

æ¯ 10 ç§’æŠ“å–ä¸€æ¬¡ï¼Œå‡å»æ‰§è¡Œè€—æ—¶ä¿æŒèŠ‚å¥ç¨³å®šã€‚

---

## ğŸ§° å…­ã€å¼‚å¸¸ä¸é€€å‡ºæ§åˆ¶

`if __name__ == '__main__':     try:         main()     except KeyboardInterrupt:         logger.info('ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡º')`

æ”¯æŒæ‰‹åŠ¨ Ctrl+C ç»ˆæ­¢è¿›ç¨‹ï¼Œé˜²æ­¢å¼‚å¸¸æŠ¥é”™é€€å‡ºã€‚

---

## ğŸ§  ä¸ƒã€æ”¹è¿›å»ºè®®ï¼ˆæ›´é«˜çº§ï¼‰

|æ–¹å‘|å»ºè®®|
|---|---|
|ğŸ—ƒï¸ æ•°æ®åº“ä¼˜åŒ–|ç”¨ `ON CONFLICT` ç®€åŒ–é€»è¾‘ï¼ˆPG â‰¥9.5ï¼‰|
|ğŸ§© è°ƒåº¦ä¼˜åŒ–|å¯ç”¨ `schedule` æˆ– `apscheduler` æ›¿ä»£ `while True`|
|ğŸ§¾ å†å²å½’æ¡£|å¢åŠ â€œæ¯æ—¥æ±‡ç‡å†å²è¡¨â€å¤‡ä»½åŠŸèƒ½|
|ğŸ’¬ å‘Šè­¦ç³»ç»Ÿ|æŠ“å–å¤±è´¥å¯ç”¨é’‰é’‰/é‚®ä»¶é€šçŸ¥|
|âš™ï¸ å¹¶å‘æŠ“å–|ç”¨çº¿ç¨‹æ± æˆ–å¼‚æ­¥IOæ‰©å±•è‡³å¤šä¸ª API|

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ æ”¹é€ æˆ **é€šç”¨â€œå®šæ—¶æŠ“å– API â†’ PostgreSQLâ€æ¨¡æ¿**ï¼Ÿ  
æ¯”å¦‚ä½ æ¢æˆå¤©æ°”æ¥å£æˆ–å…¶ä»– APIï¼Œåªéœ€æ”¹ URL å’Œå­—æ®µæ˜ å°„å³å¯è‡ªåŠ¨å…¥åº“ã€‚
```

```
# import requests
# import json
# import logging
# import threading
# import psycopg2
# from psycopg2 import sql
# from psycopg2.extras import execute_batch
#
# # é…ç½®æ—¥å¿—
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)s - %(message)s',
#     handlers=[
#         logging.FileHandler('spider_amap_weather.log'),
#         logging.StreamHandler()
#     ]
# )
# logger = logging.getLogger(__name__)
#
# db_params = {
#     "host": "192.168.200.30",
#     "port": "5432",
#     "database": "spider_db",
#     "user": "postgres",
#     "password": "root"
# }
#
# # API URL
# CMB_EXCHANGE_RATE_URL = 'https://fx.cmbchina.com/api/v1/fx/rate'
#
# headers = {
#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
#     'Accept': 'application/json, text/plain, */*',
#     'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
# }
#
#
# def fetch_data_2postgresql():
#     resp = requests.get(CMB_EXCHANGE_RATE_URL)
#     resp_json = resp.json()
#     res = []
#     for i in resp_json['body']:
#         ccy_zh = i['ccyNbr']
#         ccy_en = i['ccyNbrEng'].split(' ')[1]
#         date_part = i['ratDat'].replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '')
#         time_part = i['ratTim']
#         ct_time = f"{date_part} {time_part}"
#         data = {
#             "ccy_zh": i['ccyNbr'],
#             "ccy_en": i['ccyNbrEng'].split(' ')[1],
#             "rtbBid": i['rtbBid'],
#             "rthOfr": i['rthOfr'],
#             "rtcOfr": i['rtcOfr'],
#             "rthBid": i['rthBid'],
#             "rtcBid": i['rtcBid'],
#             "ratTim": i['ratTim'],
#             "ratDat": i['ratDat']
#         }
#         res.append((ccy_zh, ccy_en, ct_time, json.dumps(data)))
#     if not res:
#         logger.info("No valid data to insert")
#         return 0
#     with psycopg2.connect(**db_params) as conn:
#         with conn.cursor() as cursor:
#             # SQLæ’å…¥è¯­å¥
#             insert_query = sql.SQL("""
#                     INSERT INTO public.spider_exchange_rate_dtl (ccy_zh, ccy_en,ct_time, data)
#                     VALUES (%s, %s, %s, %s)
#                 """)
#
#             # æ‰¹é‡æ’å…¥æ•°æ®
#             execute_batch(cursor, insert_query, res)
#             conn.commit()
#
#     logger.info(f"æˆåŠŸæ’å…¥ {len(res)} æ¡è®°å½•")
#
#
# def main():
#     fetch_data_2postgresql()
#     threading.Timer(10, main).start()
#
#
# if __name__ == '__main__':
#     main()



#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‹›å•†é“¶è¡Œå¤–æ±‡ç‰Œä»· Â· æ¯ 10 ç§’å¢é‡æ›´æ–°ï¼ˆä½ç‰ˆæœ¬ PostgreSQL å…¼å®¹ï¼‰
å…ˆåˆ æ—§è¡¨å†é‡å»ºï¼Œä¿è¯ç»“æ„æœ€æ–°
"""
import time
import requests
import json
import logging
import threading
import psycopg2

# ---------- æ—¥å¿— ----------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler('spider_amap_weather.log'),
              logging.StreamHandler()])
logger = logging.getLogger(__name__)

# ---------- æ•°æ®åº“ ----------
DB_CFG = dict(host='192.168.200.30', port=5432, database='spider_db',
              user='postgres', password='root')

# ---------- å»ºåº“+åˆ è¡¨+å»ºè¡¨ ----------
def init_db():
    # 1. å»ºåº“ï¼ˆéäº‹åŠ¡ï¼‰
    init = {**DB_CFG, 'database': 'postgres'}
    conn = psycopg2.connect(**init)
    conn.set_session(autocommit=True)
    with conn.cursor() as cur:
        cur.execute("SELECT 1 FROM pg_database WHERE datname='spider_db';")
        if not cur.fetchone():
            cur.execute("CREATE DATABASE spider_db;")
            logger.info('å·²è‡ªåŠ¨åˆ›å»ºæ•°æ®åº“ spider_db')
    conn.close()

    # 2. åˆ æ—§è¡¨å†å»ºæ–°è¡¨
    with psycopg2.connect(**DB_CFG) as conn:
        with conn.cursor() as cur:
            cur.execute("DROP TABLE IF EXISTS public.spider_exchange_rate_dtl;")
            cur.execute("""
                CREATE TABLE public.spider_exchange_rate_dtl (
                    id      BIGSERIAL PRIMARY KEY,
                    ccy_zh  VARCHAR(20) NOT NULL,
                    ccy_en  VARCHAR(3)  NOT NULL,
                    ct_time TIMESTAMP   NOT NULL,
                    data    TEXT        NOT NULL
                );
            """)
            cur.execute("""
                CREATE UNIQUE INDEX uk_ccy_time
                ON public.spider_exchange_rate_dtl (ccy_en, ct_time);
            """)
            logger.info('å·²åˆ é™¤æ—§è¡¨å¹¶é‡æ–°åˆ›å»º')

# ---------- æŠ“å–+ä½ç‰ˆæœ¬å…¼å®¹å†™å…¥ ----------
def fetch():
    url = 'https://fx.cmbchina.com/api/v1/fx/rate'
    try:
        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        resp.raise_for_status()
        body = resp.json()['body']
    except Exception as e:
        logger.error('æŠ“å–å¤±è´¥: %s', e)
        return 0

    res = []
    for i in body:
        date_part = i['ratDat'].replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '')
        ct_time = f"{date_part} {i['ratTim']}"
        res.append((
            i['ccyNbr'],
            i['ccyNbrEng'].split()[-1],
            ct_time,
            json.dumps(i, ensure_ascii=False)
        ))

    if not res:
        logger.info('æ— æœ‰æ•ˆæ•°æ®')
        return 0

    # ä½ç‰ˆæœ¬å…¼å®¹ï¼šUPDATE â†’ INSERT
    with psycopg2.connect(**DB_CFG) as conn:
        with conn.cursor() as cur:
            for row in res:
                cur.execute("""
                    UPDATE public.spider_exchange_rate_dtl
                    SET ccy_zh = %s, data = %s
                    WHERE ccy_en = %s AND ct_time = %s;
                """, (row[0], row[3], row[1], row[2]))

                cur.execute("""
                    INSERT INTO public.spider_exchange_rate_dtl (ccy_zh, ccy_en, ct_time, data)
                    SELECT %s, %s, %s, %s
                    WHERE NOT EXISTS (
                        SELECT 1 FROM public.spider_exchange_rate_dtl
                        WHERE ccy_en = %s AND ct_time = %s
                    );
                """, row + (row[1], row[2]))     # åä¸¤ä¸ªç»™ NOT EXISTS ç”¨

            logger.info('æˆåŠŸå¤„ç† %s æ¡è®°å½•', len(res))
    return len(res)

# ---------- 10 ç§’å¾ªç¯ ----------
def main():
    init_db()
    while True:
        t0 = time.time()
        fetch()
        time.sleep(max(0, 10 - (time.time() - t0)))

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        logger.info('ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡º')
```


# 6. å®‰è£… FineReport 11 Version
![[Pasted image 20251006165920.png]]![[Pasted image 20251006165937.png]]