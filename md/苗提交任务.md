# minio
2025-10-1
# Minio 安装

可安装单节点或分布式的 minio , 本地部署或者 docker 都可 minio 上传⼀个 csv ⽂件, 使⽤ hive 映射该⽂件为表, hive 可查询, 能实现 ds 分区为佳, 使⽤时间分区, ds=20250925


### 一、环境准备

1. **前置条件**

- 已安装 CDH 单节点集群（确保操作系统为 CentOS 7/8 或 Ubuntu 18.04+）

- 关闭防火墙或开放 MinIO 所需端口（默认 9000 用于 API，9001 用于控制台）

```bash

# CentOS 关闭防火墙

systemctl stop firewalld

systemctl disable firewalld

# Ubuntu 关闭防火墙

ufw disable

```

- 确保系统有足够的磁盘空间（建议单独挂载一块磁盘用于 MinIO 数据存储，如`/data/minio`）

### 二、安装 MinIO 服务

#### 1. 创建数据目录与用户

bash

```bash

# 创建 MinIO 数据存储目录（建议使用独立磁盘挂载点）

mkdir -p /data/minio

chmod -R 777 /data/minio # 赋予读写权限

# 创建专用用户（可选，增强安全性）

useradd -m minio

chown -R minio:minio /data/minio（不选）

```

#### 2. 下载 MinIO 二进制文件

MinIO 基于 Go 语言开发，单文件部署无需依赖，直接下载即可：

```bash

# 切换到 /usr/local/bin 目录（系统可执行路径）

cd /usr/local/bin

# 下载 MinIO 二进制文件（选择对应系统版本，这里以 Linux amd64 为例）

wget https://dl.min.io/server/minio/release/linux-amd64/minio

# 赋予执行权限

chmod +x minio

# 验证是否可执行

minio --version # 输出类似：minio version RELEASE.2024-05-28T17-19-04Z

```

#### 3. 配置环境变量（密钥与存储路径）

```bash

# 创建 MinIO 配置文件

vi /etc/profile.d/minio.sh

# 添加以下内容（设置管理员账号密码和数据目录）

export MINIO_ROOT_USER="minioadmin" # 管理用户名

export MINIO_ROOT_PASSWORD="minioadmin" # 管理密码（生产环境建议复杂密码）

export MINIO_VOLUMES="/data/minio" # 数据存储目录

export MINIO_OPTS="--console-address :9001" # 控制台端口（API 端口默认 9000）

# 使配置生效

source /etc/profile.d/minio.sh

```

### 三、配置系统服务（开机自启）

为便于管理，将 MinIO 注册为系统服务：

#### 1. 创建服务文件

```bash

vi /etc/systemd/system/minio.service

```

#### 2. 写入服务配置

```ini

（不要用这个）

[Unit]

Description=MinIO Object Storage Service

Documentation=https://min.io/docs/minio/linux/index.html

Wants=network-online.target

After=network-online.target

[Service]

User=minio # 运行用户（若创建了专用用户）

Group=minio # 运行用户组

EnvironmentFile=/etc/profile.d/minio.sh # 引用环境变量配置

ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES

Restart=always # 故障自动重启

RestartSec=5 # 重启间隔 5 秒

[Install]

WantedBy=multi-user.target

（使用这个文件）

[Unit]

Description=MinIO Object Storage Service

Documentation=https://min.io/docs/minio/linux/index.html

Wants=network-online.target

After=network-online.target

[Service]

# 用 root 启动，不再指定 minio 用户

ExecStart=/usr/local/bin/minio server /data --console-address ":9001"

Restart=always

RestartSec=5

LimitNOFILE=65536

Environment="MINIO_ROOT_USER=admin"

Environment="MINIO_ROOT_PASSWORD=admin123456"

[Install]

WantedBy=multi-user.target

```

1. 确认 MinIO 二进制是否存在

`which minio ls -lh /usr/local/bin/minio`

如果没有，就去下载最新版本：

```bash

`wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio mv minio /usr/local/bin/`

```

#### 3. 启动并设置自启

```bash

# 重载系统服务

systemctl daemon-reload

# 启动 MinIO

systemctl start minio

# 查看状态（确保 Active: active (running)）

systemctl status minio

# 设置开机自启

systemctl enable minio

```

### 四、访问 MinIO 控制台

1. **通过浏览器访问**打开`http://CDH节点IP:9001`，使用配置的账号密码（`minioadmin/minioadmin`）登录。

2. **创建存储桶（Bucket）**

- 登录后点击左侧**Buckets**→**Create Bucket**

- 输入桶名（如`cdh-minio-bucket`），保持默认配置，点击**Create**。

### 五、CDH 集成 MinIO（可选）

若需让 CDH 组件（如 Spark、Hive）访问 MinIO，需配置 S3 兼容参数：

#### 1. 配置 Core Site

在 CDH 管理界面（Cloudera Manager）→**HDFS**→**配置**→ 搜索`core-site.xml`，添加以下属性：

```xml

<property>

<name>fs.s3a.endpoint</name>

<value>http://CDH节点IP:9000</value> <!-- MinIO API 地址 -->

</property>

<property>

<name>fs.s3a.access.key</name>

<value>minioadmin</value> <!-- MinIO 用户名 -->

</property>

<property>

<name>fs.s3a.secret.key</name>

<value>minioadmin</value> <!-- MinIO 密码 -->

</property>

<property>

<name>fs.s3a.path.style.access</name>

<value>true</value> <!-- 启用路径风格访问（避免 DNS 配置） -->

</property>

<property>

<name>fs.s3a.connection.ssl.enabled</name>

<value>false</value> <!-- 单节点环境可关闭 SSL -->

</property>

```

#### 2. 验证集成

通过 HDFS 命令访问 MinIO：

```bash

# 列出 MinIO 中的桶

hadoop fs -ls s3a://

# 上传文件到 MinIO 桶

hadoop fs -put /etc/profile s3a://cdh-minio-bucket/

```

### 六、常见问题排查

1. **无法访问控制台**

- 检查端口是否开放：`netstat -tulpn | grep 9001` 或 `ss -tulpn | grep 9001

`

- 查看日志：`journalctl -u minio -f`

2. **CDH 组件访问失败**

- 确认`core-site.xml`配置中的 endpoint、账号密码是否正确

- 测试网络连通性：`curl http://CDH节点IP:9000`（应返回 XML 格式的错误信息）

3. **权限问题**

- 确保数据目录`/data/minio`权限正确（`chmod 777`或归属 minio 用户）

通过以上步骤，即可在 CDH 单节点环境中完成 MinIO 安装，并实现与 Hadoop 生态的集成，用于存储非结构化数据或作为计算框架的输入输出存储。

## minio 上传⼀个 csv ⽂件, 使⽤ hive 映射该⽂件为表, hive 可查询, 能实现 ds 分区为佳, 使⽤时间分区, ds=20250925

``` sql

CREATE EXTERNAL TABLE my_csv_table 

( 

CRIM double, 

ZN double, 

INDUS double, 

CHAS double, 

NOX double, 

RM double, 

AGE double, 

DIS double, 

RAD double, 

TAX double, 

PTRATIO double, 

B double, 

LSTAT double, 

TARGET double 

) 

PARTITIONED BY (ds STRING) 

ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' 

STORED AS TEXTFILE 

LOCATION 's3a://first-bucket/'; 

ALTER TABLE my_csv_table ADD PARTITION (ds='20250925') 

LOCATION 's3a://first-bucket/ds=20250925/'; 

SELECT * FROM my_csv_table WHERE ds='20250925'; 

MSCK REPAIR TABLE my_csv_table;


```






#  2. CDH 安装 hbase
# 创建 hbase 表, 数据由 Flink 写⼊ FlinkAPI 或者 FlinkSQL 都可以
#  hbase rowkey 的设计, 符合企业原则
# hbase 数据进⼊后, 使⽤ hive 和 hbase 的映射表进⾏映射, 数据需要可查询, 实现可分区为佳


```
1. 登录 cdh1
    
    bash
    
    
    
    ```bash
    ssh root@192.168.200.30
    ```
    
2. 修复 classpath（只做一次）
    
    bash
    
    
    
    ```bash
    echo 'export HBASE_CLASSPATH=$(find /opt/cloudera/parcels/CDH -name "*.jar" | tr '\n' ':')' >> ~/.bashrc
    source ~/.bashrc
    ```
    
3. 进 hbase shell
    
    bash
    
    
    
    ```bash
    hbase shell
    ```
    
4. 依次粘贴（**# 后面是注释，不用粘**）
    
    ruby
    
    
    
    ```ruby
    # 1. 命名空间
    create_namespace 'dim'
    
    # 2. 建表（散列+ds 后缀，预分区 16 个）
    create 'dim:dim_activity_full',
           {NAME => 'f', COMPRESSION => 'SNAPPY', VERSIONS => 1},
           {SPLITS => ['1000|','2000|','3000|','4000|','5000|','6000|','7000|','8000|','9000|','a000|','b000|','c000|','d000|','e000|','f000|']}
    
    # 3. 验证
    describe 'dim:dim_activity_full'
    list_namespace_tables 'dim'
    ```
    
5. 看到 **15 个 split** 和 **SNAPPY** 即成功，输入
    
    ruby
    
    
    
    ```ruby
    exit
    ```
    

---

2. 第二步：在 cdh1 建 Hive 映射表（3 分钟）
    

---

1. 还在 cdh1，切到 hive 用户（root 也可）
    
    bash
    
    
    
    ```bash
    beeline -u jdbc:hive2://cdh1:10000 -n hive
    ```
    
2. 一次性粘贴
    
    sql
    
    
    
    ```sql
    CREATE EXTERNAL TABLE dim_activity_full_hbase(
      activity_rule_id    string,
      activity_id         string,
      activity_name       string,
      activity_type_code  string,
      activity_type_name  string,
      activity_desc       string,
      start_time          string,
      end_time            string,
      create_time         string,
      condition_amount    decimal(16,2),
      condition_num       bigint,
      benefit_amount      decimal(16,2),
      benefit_discount    decimal(16,2),
      benefit_rule        string,
      benefit_level       string
    )
    PARTITIONED BY (ds string)
    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
    WITH SERDEPROPERTIES (
      "hbase.columns.mapping" =
      ":key,f:activity_id,f:activity_name,f:activity_type_code,f:activity_type_name,f:activity_desc,f:start_time,f:end_time,f:create_time,f:condition_amount,f:condition_num,f:benefit_amount,f:benefit_discount,f:benefit_rule,f:benefit_level"
    )
    TBLPROPERTIES (
      "hbase.table.name" = "dim:dim_activity_full",
      "hive.hbase.rowkey.part" = "2",
      "hive.hbase.rowkey.separator" = "|"
    );
    ```
    
3. 退出 beeline
    
    sql
    
    
    
    ```sql
    !quit
    ```
    

---

3. 第三步：在 cdh1 提交 Flink 作业（15 分钟）
    

---

3.1 准备源码（cdh1 任意目录）

bash



```bash
mkdir -p ~/flink_job && cd ~/flink_job
```

3.2 新建 `ActivityToHBaseSQL.java`

java



```java
import org.apache.flink.table.api.*;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class ActivityToHBaseSQL {
    public static void main(String[] args) {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.enableCheckpointing(30_000);
        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

        // 1. Kafka 源
        tEnv.executeSql("CREATE TABLE kafka_activity ("
                + "  activity_rule_id STRING,"
                + "  activity_id      STRING,"
                + "  activity_name    STRING,"
                + "  activity_type_code STRING,"
                + "  activity_type_name STRING,"
                + "  activity_desc    STRING,"
                + "  start_time       STRING,"
                + "  end_time         STRING,"
                + "  create_time      STRING,"
                + "  condition_amount DECIMAL(16,2),"
                + "  condition_num    BIGINT,"
                + "  benefit_amount   DECIMAL(16,2),"
                + "  benefit_discount DECIMAL(16,2),"
                + "  benefit_rule     STRING,"
                + "  benefit_level    STRING,"
                + "  ds               STRING"
                + ") WITH ("
                + "  'connector' = 'kafka',"
                + "  'topic' = 'activity_topic',"
                + "  'properties.bootstrap.servers' = '192.168.200.30:9092,192.168.200.31:9092,192.168.200.32:9092',"
                + "  'format' = 'json'"
                + ")");

        // 2. HBase 目标
        tEnv.executeSql("CREATE TABLE dim_activity_full_hbase ("
                + "  rk STRING,"
                + "  f  ROW<activity_id STRING, activity_name STRING, activity_type_code STRING, activity_type_name STRING, activity_desc STRING, start_time STRING, end_time STRING, create_time STRING, condition_amount DECIMAL(16,2), condition_num BIGINT, benefit_amount DECIMAL(16,2), benefit_discount DECIMAL(16,2), benefit_rule STRING, benefit_level STRING>,"
                + "  PRIMARY KEY (rk) NOT ENFORCED"
                + ") WITH ("
                + "  'connector' = 'hbase-1.4',"
                + "  'table-name' = 'dim:dim_activity_full',"
                + "  'zookeeper.quorum' = '192.168.200.30:2181,192.168.200.31:2181,192.168.200.32:2181',"
                + "  'zookeeper.znode.parent' = '/hbase',"
                + "  'sink.buffer-flush.max-rows' = '500',"
                + "  'sink.buffer-flush.interval' = '1s'"
                + ")");

        // 3. 写入
        tEnv.executeSql("INSERT INTO dim_activity_full_hbase "
                + "SELECT "
                + "  LPAD(LOWER(HEX(MD5(activity_rule_id))),4,'0') || '|' || activity_rule_id || '|' || ds AS rk,"
                + "  ROW(activity_id, activity_name, activity_type_code, activity_type_name, activity_desc, start_time, end_time, create_time, condition_amount, condition_num, benefit_amount, benefit_discount, benefit_rule, benefit_level) AS f "
                + "FROM kafka_activity");
    }
}
```

3.3 编译（用 CDH 自带的 flink-shaded-hbase）

bash



```bash
# 把依赖 jar 放到 lib 目录
cp /opt/cloudera/parcels/CDH/jars/hbase-* ~/flink_job/lib/
cp /opt/cloudera/parcels/FLINK/lib/flink-table-* ~/flink_job/lib/

# 编译
javac -cp "lib/*" ActivityToHBaseSQL.java
jar -cvf activity.jar ActivityToHBaseSQL.class
```

3.4 提交作业（仍在 cdh1）

bash



```bash
export HADOOP_CONF_DIR=/etc/hadoop/conf
flink run -m yarn-cluster -ynm activity2hbase -ys 2 -yjm 1024 -ytm 2048 activity.jar
```

看到 **"Submitted application application_xxx"** 即成功。

---

4. 第四步：发测试数据（任意节点，2 分钟）


---

bash
```bash
kafka-console-producer --broker-list 192.168.200.30:9092 --topic activity_topic
```

粘一条：

JSON



```json
{"activity_rule_id":"rule_123","activity_id":"act_456","activity_name":"双11满减","activity_type_code":"COUPON","activity_type_name":"优惠券","activity_desc":"满1000减200","start_time":"2025-10-01 00:00:00","end_time":"2025-10-07 23:59:59","create_time":"2025-09-30 12:00:00","condition_amount":1000.00,"condition_num":1,"benefit_amount":200.00,"benefit_discount":null,"benefit_rule":"CASH","benefit_level":"A","ds":"2025-10-06"}
```

Ctrl+C 退出。

---

5. 第五步：Hive 查询（1 分钟）


---

1. 加分区

   bash
    ```bash
    beeline -u jdbc:hive2://cdh1:10000 -n hive -e "ALTER TABLE dim_activity_full_hbase ADD PARTITION (ds='2025-10-06');"
    ```

2. 查询

   sql
    ```sql
    beeline -u jdbc:hive2://cdh1:10000 -n hive
    SELECT * FROM dim_activity_full_hbase WHERE ds='2025-10-06' AND activity_type_code='COUPON';
    ```

   能返回刚才那条数据即 **全流程打通**。




# 3. python spider 爬⾍

# 3.1 爬取中国⽓象数据, 数据要求 10 s 更新⼀次，增量更新

# 3.2 爬取中国外汇当⽇市场数据数据 10 s 更新⼀次，增量更新

# ps : 环境可移植, 代码需要在 linux 中可运⾏, 使⽤ conda 环境进⾏配置, 配置反爬⼿段进⾏预防
```

## 3.1 爬取中国⽓象数据, 数据要求 10 s 更新⼀次，增量更新

"""
邓州 7 天天气 · 每 10 秒增量更新（兼容 PG<9.5）
"""
import time, signal, sys
import requests, psycopg2, re
from lxml import etree

# ---------- 数据库 ----------
DB_CFG = dict(
    dbname='postgres',
    user='postgres',
    password='root',
    host='192.168.200.30',
    port=5432
)

# ---------- SQL ----------
CREATE_TABLE_SQL = """
CREATE TABLE IF NOT EXISTS public.weather7days (
    date        DATE PRIMARY KEY,
    weekday     VARCHAR(3),
    weather     VARCHAR(10),
    wind_dir    VARCHAR(10),
    wind_lvl    VARCHAR(10),
    temp_high   SMALLINT,
    temp_low    SMALLINT
);
"""

# 低版本兼容：先更新，没命中再插入
UPSERT_SQL = """
UPDATE public.weather7days
SET weekday     = %(weekday)s,
    weather     = %(weather)s,
    wind_dir    = %(wind_dir)s,
    wind_lvl    = %(wind_lvl)s,
    temp_high   = %(temp_high)s,
    temp_low    = %(temp_low)s
WHERE date = %(date)s;
INSERT INTO public.weather7days
(date, weekday, weather, wind_dir, wind_lvl, temp_high, temp_low)
SELECT %(date)s, %(weekday)s, %(weather)s, %(wind_dir)s, %(wind_lvl)s,
       %(temp_high)s, %(temp_low)s
WHERE NOT EXISTS (SELECT 1 FROM public.weather7days WHERE date = %(date)s);
"""

# ---------- 抓取 ----------
def crawl():
    url = 'https://weather.cma.cn/web/weather/57178.html'
    headers = {'User-Agent': 'Mozilla/5.0'}
    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()
    resp.encoding = 'utf-8'

    txt = ''.join(etree.HTML(resp.text)
                  .xpath('/html/body/div[1]/div[2]/div[1]/div[1]/div[2]')[0]
                  .itertext())

    pat = re.compile(r'星期([\u4e00-\u9fa5])\s*(\d{2}/\d{2})\s*'
                     r'([\u4e00-\u9fa5]+)\s*'
                     r'([\u4e00-\u9fa5]{2}风)\s*'
                     r'([\u4e00-\u9fa5\d~级]+)\s*'
                     r'(\d{1,2})℃\s*(\d{1,2})℃', re.S)

    rows = []
    for w, d, we, wd, wl, h, l in pat.findall(txt):
        mon, day = d.split('/')
        rows.append(dict(
            date=f'2025-{int(mon):02d}-{int(day):02d}',
            weekday=w,
            weather=we,
            wind_dir=wd,
            wind_lvl=wl,
            temp_high=int(h),
            temp_low=int(l)
        ))
    return rows

# ---------- 主循环 ----------
def main():
    with psycopg2.connect(**DB_CFG) as conn:
        with conn.cursor() as cur:
            cur.execute(CREATE_TABLE_SQL)

    while True:
        try:
            t0 = time.time()
            data = crawl()
            with psycopg2.connect(**DB_CFG) as conn:
                with conn.cursor() as cur:
                    for row in data:
                        cur.execute(UPSERT_SQL, row)
                    print(f'{time.strftime("%Y-%m-%d %H:%M:%S")} '
                          f'已更新 {len(data)} 条记录')
            time.sleep(max(0, 10 - (time.time() - t0)))
        except KeyboardInterrupt:
            print('用户中断，退出')
            sys.exit(0)
        except Exception as e:
            print('抓取异常：', e)
            time.sleep(10)

if __name__ == '__main__':
    signal.signal(signal.SIGINT, lambda *args: sys.exit(0))
    main()
```
我将爬取的数据存储到pg里，然后没十秒增量同步







# 3.2 爬取中国外汇当⽇市场数据数据 10 s 更新⼀次，增量更新

```
> **每 10 秒抓取一次招商银行外汇牌价 API（`https://fx.cmbchina.com/api/v1/fx/rate`）并写入 PostgreSQL 数据库。**

特点：

- 自动建库建表；
    
- 低版本 PostgreSQL 兼容（不依赖 `ON CONFLICT`）；
    
- 每条数据包含币种、中文名、时间戳、完整 JSON；
    
- 每 10 秒自动更新；
    
- 带日志记录、异常捕获。
    

---

## 🏗️ 二、模块结构分解

### 1️⃣ 日志配置

`logging.basicConfig(     level=logging.INFO,     format='%(asctime)s - %(levelname)s - %(message)s',     handlers=[         logging.FileHandler('spider_amap_weather.log'),         logging.StreamHandler()     ] )`

- 同时写入文件 `spider_amap_weather.log` 和终端；
    
- 格式：`时间 - 日志级别 - 消息`;
    
- 全局日志对象 `logger` 用于后续输出。
    

---

### 2️⃣ 数据库配置

`DB_CFG = dict(     host='192.168.200.30',     port=5432,     database='spider_db',     user='postgres',     password='root' )`

简单明了，便于改环境时统一维护。

---

## 🧱 三、数据库初始化函数 `init_db()`

这部分非常实用，自动处理数据库不存在的情况 👇

`def init_db():     init = {**DB_CFG, 'database': 'postgres'}     conn = psycopg2.connect(**init)     conn.set_session(autocommit=True)`

这里连的是默认数据库 `postgres`（系统库），因为目标数据库 `spider_db` 可能还没创建。

---

### 3.1 检查数据库是否存在

`cur.execute("SELECT 1 FROM pg_database WHERE datname='spider_db';") if not cur.fetchone():     cur.execute("CREATE DATABASE spider_db;")     logger.info('已自动创建数据库 spider_db')`

- 如果数据库不存在则自动 `CREATE DATABASE`；
    
- 并打印日志提示。
    

---

### 3.2 删除旧表并重建

`with psycopg2.connect(**DB_CFG) as conn:     with conn.cursor() as cur:         cur.execute("DROP TABLE IF EXISTS public.spider_exchange_rate_dtl;")         cur.execute("""             CREATE TABLE public.spider_exchange_rate_dtl (                 id      BIGSERIAL PRIMARY KEY,                 ccy_zh  VARCHAR(20) NOT NULL,                 ccy_en  VARCHAR(3)  NOT NULL,                 ct_time TIMESTAMP   NOT NULL,                 data    TEXT        NOT NULL             );         """)         cur.execute("""             CREATE UNIQUE INDEX uk_ccy_time             ON public.spider_exchange_rate_dtl (ccy_en, ct_time);         """)`

📌 **作用：**

- 删除旧表，防止结构不一致；
    
- 重新建表；
    
- 创建唯一索引 `(ccy_en, ct_time)`，保证每个币种每个时间点唯一。
    

---

## 🌐 四、抓取函数 `fetch()`

### 4.1 请求接口

`url = 'https://fx.cmbchina.com/api/v1/fx/rate' resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10) resp.raise_for_status() body = resp.json()['body']`

- 调用招商银行外汇API；
    
- 检查HTTP状态；
    
- 解析JSON；
    
- 提取主数据体 `body`（列表）。
    

---

### 4.2 数据解析

`for i in body:     date_part = i['ratDat'].replace('年', '-').replace('月', '-').replace('日', '')     ct_time = f"{date_part} {i['ratTim']}"     res.append((         i['ccyNbr'],         i['ccyNbrEng'].split()[-1],         ct_time,         json.dumps(i, ensure_ascii=False)     ))`

提取字段：

|字段|说明|
|---|---|
|`ccy_zh`|中文币种名|
|`ccy_en`|英文币种（取英文名最后一个单词）|
|`ct_time`|合并日期 + 时间字符串|
|`data`|原始JSON序列化（保留全部字段）|

---

### 4.3 插入或更新（低版本兼容写法）

`for row in res:     cur.execute("""         UPDATE public.spider_exchange_rate_dtl         SET ccy_zh = %s, data = %s         WHERE ccy_en = %s AND ct_time = %s;     """, (row[0], row[3], row[1], row[2]))      cur.execute("""         INSERT INTO public.spider_exchange_rate_dtl (ccy_zh, ccy_en, ct_time, data)         SELECT %s, %s, %s, %s         WHERE NOT EXISTS (             SELECT 1 FROM public.spider_exchange_rate_dtl             WHERE ccy_en = %s AND ct_time = %s         );     """, row + (row[1], row[2]))`

🔥 重点：

- **没有使用 PostgreSQL 9.5+ 的 `ON CONFLICT DO UPDATE` 语法，**
    
- 而是手动写了两步：
    
    1. `UPDATE`（若存在则更新）；
        
    2. `INSERT ... WHERE NOT EXISTS`（若不存在则插入）。
        

✅ 兼容 PostgreSQL 9.2/9.3/9.4 等旧版本。

---

## 五、循环逻辑

`while True:     t0 = time.time()     fetch()     time.sleep(max(0, 10 - (time.time() - t0)))`

每 10 秒抓取一次，减去执行耗时保持节奏稳定。

---

## 🧰 六、异常与退出控制

`if __name__ == '__main__':     try:         main()     except KeyboardInterrupt:         logger.info('用户中断，退出')`

支持手动 Ctrl+C 终止进程，防止异常报错退出。

---

## 🧠 七、改进建议（更高级）

|方向|建议|
|---|---|
|🗃️ 数据库优化|用 `ON CONFLICT` 简化逻辑（PG ≥9.5）|
|🧩 调度优化|可用 `schedule` 或 `apscheduler` 替代 `while True`|
|🧾 历史归档|增加“每日汇率历史表”备份功能|
|💬 告警系统|抓取失败可用钉钉/邮件通知|
|⚙️ 并发抓取|用线程池或异步IO扩展至多个 API|

---

是否希望我帮你改造成 **通用“定时抓取 API → PostgreSQL”模板**？  
比如你换成天气接口或其他 API，只需改 URL 和字段映射即可自动入库。
```

```
# import requests
# import json
# import logging
# import threading
# import psycopg2
# from psycopg2 import sql
# from psycopg2.extras import execute_batch
#
# # 配置日志
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)s - %(message)s',
#     handlers=[
#         logging.FileHandler('spider_amap_weather.log'),
#         logging.StreamHandler()
#     ]
# )
# logger = logging.getLogger(__name__)
#
# db_params = {
#     "host": "192.168.200.30",
#     "port": "5432",
#     "database": "spider_db",
#     "user": "postgres",
#     "password": "root"
# }
#
# # API URL
# CMB_EXCHANGE_RATE_URL = 'https://fx.cmbchina.com/api/v1/fx/rate'
#
# headers = {
#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
#     'Accept': 'application/json, text/plain, */*',
#     'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
# }
#
#
# def fetch_data_2postgresql():
#     resp = requests.get(CMB_EXCHANGE_RATE_URL)
#     resp_json = resp.json()
#     res = []
#     for i in resp_json['body']:
#         ccy_zh = i['ccyNbr']
#         ccy_en = i['ccyNbrEng'].split(' ')[1]
#         date_part = i['ratDat'].replace('年', '-').replace('月', '-').replace('日', '')
#         time_part = i['ratTim']
#         ct_time = f"{date_part} {time_part}"
#         data = {
#             "ccy_zh": i['ccyNbr'],
#             "ccy_en": i['ccyNbrEng'].split(' ')[1],
#             "rtbBid": i['rtbBid'],
#             "rthOfr": i['rthOfr'],
#             "rtcOfr": i['rtcOfr'],
#             "rthBid": i['rthBid'],
#             "rtcBid": i['rtcBid'],
#             "ratTim": i['ratTim'],
#             "ratDat": i['ratDat']
#         }
#         res.append((ccy_zh, ccy_en, ct_time, json.dumps(data)))
#     if not res:
#         logger.info("No valid data to insert")
#         return 0
#     with psycopg2.connect(**db_params) as conn:
#         with conn.cursor() as cursor:
#             # SQL插入语句
#             insert_query = sql.SQL("""
#                     INSERT INTO public.spider_exchange_rate_dtl (ccy_zh, ccy_en,ct_time, data)
#                     VALUES (%s, %s, %s, %s)
#                 """)
#
#             # 批量插入数据
#             execute_batch(cursor, insert_query, res)
#             conn.commit()
#
#     logger.info(f"成功插入 {len(res)} 条记录")
#
#
# def main():
#     fetch_data_2postgresql()
#     threading.Timer(10, main).start()
#
#
# if __name__ == '__main__':
#     main()



#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
招商银行外汇牌价 · 每 10 秒增量更新（低版本 PostgreSQL 兼容）
先删旧表再重建，保证结构最新
"""
import time
import requests
import json
import logging
import threading
import psycopg2

# ---------- 日志 ----------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler('spider_amap_weather.log'),
              logging.StreamHandler()])
logger = logging.getLogger(__name__)

# ---------- 数据库 ----------
DB_CFG = dict(host='192.168.200.30', port=5432, database='spider_db',
              user='postgres', password='root')

# ---------- 建库+删表+建表 ----------
def init_db():
    # 1. 建库（非事务）
    init = {**DB_CFG, 'database': 'postgres'}
    conn = psycopg2.connect(**init)
    conn.set_session(autocommit=True)
    with conn.cursor() as cur:
        cur.execute("SELECT 1 FROM pg_database WHERE datname='spider_db';")
        if not cur.fetchone():
            cur.execute("CREATE DATABASE spider_db;")
            logger.info('已自动创建数据库 spider_db')
    conn.close()

    # 2. 删旧表再建新表
    with psycopg2.connect(**DB_CFG) as conn:
        with conn.cursor() as cur:
            cur.execute("DROP TABLE IF EXISTS public.spider_exchange_rate_dtl;")
            cur.execute("""
                CREATE TABLE public.spider_exchange_rate_dtl (
                    id      BIGSERIAL PRIMARY KEY,
                    ccy_zh  VARCHAR(20) NOT NULL,
                    ccy_en  VARCHAR(3)  NOT NULL,
                    ct_time TIMESTAMP   NOT NULL,
                    data    TEXT        NOT NULL
                );
            """)
            cur.execute("""
                CREATE UNIQUE INDEX uk_ccy_time
                ON public.spider_exchange_rate_dtl (ccy_en, ct_time);
            """)
            logger.info('已删除旧表并重新创建')

# ---------- 抓取+低版本兼容写入 ----------
def fetch():
    url = 'https://fx.cmbchina.com/api/v1/fx/rate'
    try:
        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        resp.raise_for_status()
        body = resp.json()['body']
    except Exception as e:
        logger.error('抓取失败: %s', e)
        return 0

    res = []
    for i in body:
        date_part = i['ratDat'].replace('年', '-').replace('月', '-').replace('日', '')
        ct_time = f"{date_part} {i['ratTim']}"
        res.append((
            i['ccyNbr'],
            i['ccyNbrEng'].split()[-1],
            ct_time,
            json.dumps(i, ensure_ascii=False)
        ))

    if not res:
        logger.info('无有效数据')
        return 0

    # 低版本兼容：UPDATE → INSERT
    with psycopg2.connect(**DB_CFG) as conn:
        with conn.cursor() as cur:
            for row in res:
                cur.execute("""
                    UPDATE public.spider_exchange_rate_dtl
                    SET ccy_zh = %s, data = %s
                    WHERE ccy_en = %s AND ct_time = %s;
                """, (row[0], row[3], row[1], row[2]))

                cur.execute("""
                    INSERT INTO public.spider_exchange_rate_dtl (ccy_zh, ccy_en, ct_time, data)
                    SELECT %s, %s, %s, %s
                    WHERE NOT EXISTS (
                        SELECT 1 FROM public.spider_exchange_rate_dtl
                        WHERE ccy_en = %s AND ct_time = %s
                    );
                """, row + (row[1], row[2]))     # 后两个给 NOT EXISTS 用

            logger.info('成功处理 %s 条记录', len(res))
    return len(res)

# ---------- 10 秒循环 ----------
def main():
    init_db()
    while True:
        t0 = time.time()
        fetch()
        time.sleep(max(0, 10 - (time.time() - t0)))

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        logger.info('用户中断，退出')
```


# 6. 安装 FineReport 11 Version
![[Pasted image 20251006165920.png]]![[Pasted image 20251006165937.png]]